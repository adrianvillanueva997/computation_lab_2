# -*- coding: utf-8 -*-

import os
import pickle

import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split

from Interfaces.Database import Text_Procesing, File_Manager


class Vectorizer:
    """
    Class that does all the vectorization and generation process of the whole data frame and the test/train slitted data
    """

    def __init__(self):
        """
        Class constructor
        """
        self.__dataframe = None
        self.__vectorizer = None

    def __count_vectorizer_train(self, x_train, x_test):
        """
        Convert a collection of text documents to a matrix of token counts
        :param to_array:
        """
        tp = Text_Procesing.Text_Processing()
        stop_words = set(stopwords.words('spanish'))
        cv = CountVectorizer(tokenizer=tp.tokenizer, stop_words=stop_words)
        cv.fit(x_train)
        x_train = cv.transform(x_train).toarray()
        x_test = cv.transform(x_test).toarray()
        self.__vectorizer = cv
        return x_train, x_test

    def __term_frequency_vectorizer_train(self, x_train, x_test):
        """
        Convert a collection of text documents to a matrix of token counts
        :param to_array:
        """
        tp = Text_Procesing.Text_Processing()
        stop_words = set(stopwords.words('spanish'))
        cv = TfidfVectorizer(tokenizer=tp.tokenizer, stop_words=stop_words)
        cv.fit(x_train)
        x_train = cv.transform(x_train).toarray()
        x_test = cv.transform(x_test).toarray()
        self.__vectorizer = cv
        return x_train, x_test

    def export_vectorizer(self, path, model_name):
        try:
            extension = '.vocab'
            file_name = str(model_name) + str(extension)
            full_path = os.path.join(path, file_name)
            pickle.dump(self.__vectorizer, open(full_path, "wb"))
        except Exception as e:
            print(e)

    def load_vectorizer(self, path):
        try:
            self.__vectorizer = pickle.load(open(path, 'rb'))
        except Exception as e:
            print(e)

    def get_vectorizer(self):
        return self.__vectorizer

    def generate_train_test_data(self, data_frame, vectorizer, test_size=0.1, random_state=None,
                                 train_size=None):
        """
        Generate train/test data given some vectorized reviews
        :param vectorizer:
        :param train_size: represent the proportion of the dataset to include in the test split. (if float, it should be between 0 and 1)
        :param test_size: represent the proportion of the dataset to include in the train split. (if float, it should be between 0 and 1)
        :param random_state: seed used by the random number generator. If it's none it is generated by numpy
        :return: X_train, X_test, y_train, y_test
        """
        print("vectorizer dentor de generate_train: {}".format(vectorizer))
        self.__dataframe = data_frame
        x_train, x_test, y_train, y_test = train_test_split(data_frame['reviews'],
                                                            data_frame['labels'],
                                                            test_size=test_size,
                                                            random_state=random_state,
                                                            train_size=train_size)
        if vectorizer == "count_vect":
            print("Entra en count_vect")
            x_train, x_test = self.__count_vectorizer_train(
                x_train=x_train, x_test=x_test)
        elif vectorizer == "tfid":
            print("Entra en tfid")
            x_train, x_test = self.__term_frequency_vectorizer_train(
                x_train=x_train, x_test=x_test)

        else:
            print("Entra en el else y devuelve None")
            return None

        print(x_train, x_test, y_train, y_test)

        return x_train, x_test, y_train, y_test

    def generate_unlabeled_data(self, file_names):
        unlabeled_data = self.__vectorizer.transform(
            self.__data_frame['reviews']).toarray()
        return unlabeled_data

    def update_unlabeled_dataframe(self, predicted_data):
        self.__data_frame['labels'] = predicted_data

    def plot_dataframe(self, container=None):
        if container is None:
            plot = self.__data_frame['labels'].value_counts().plot('bar')
            plt.show()
            return plot
        else:
            self.__data_frame['labels'].value_counts().plot(
                kind="bar", legend=False, ax=container)

    def export_dataframe_csv(self, path, model_name):
        try:
            extension = '.csv'
            file_name = str(model_name) + str(extension)
            full_path = os.path.join(path, file_name)
            self.__data_frame.to_csv(full_path)
        except Exception as e:
            print(e)

    def export_reviews_to_files(self, path):
        g_file_count = 1
        n_file_count = 1
        b_file_count = 1
        fm = File_Manager.File_Manager()
        g_reviews = self.__data_frame.loc[self.__data_frame['labels'] == 'G']
        b_reviews = self.__data_frame.loc[self.__data_frame['labels'] == 'B']
        n_reviews = self.__data_frame.loc[self.__data_frame['labels'] == 'N']

        if not os.path.exists(os.path.join(path, 'good')):
            os.makedirs(os.path.join(path, 'good'))
        if not os.path.exists(os.path.join(path, 'neutral')):
            os.makedirs(os.path.join(path, 'neutral'))
        if not os.path.exists(os.path.join(path, 'bad')):
            os.makedirs(os.path.join(path, 'bad'))

        for review in g_reviews['reviews']:
            fm.write_file(
                text=review, file_name=f'g_review_{str(g_file_count)}', path=os.path.join(path, 'good'))
            g_file_count += 1

        for review in b_reviews['reviews']:
            fm.write_file(
                text=review, file_name=f'b_review_{str(b_file_count)}', path=os.path.join(path, 'bad'))
            b_file_count += 1
        for review in n_reviews['reviews']:
            fm.write_file(text=review, file_name=f'n_review_{str(n_file_count)}', path=os.path.join(
                path, 'neutral'))
            n_file_count += 1
        print(
            f'[INFO] Exported: \nGood: {str(g_file_count)} \nBad: {str(b_file_count)} \nNeutral: {str(n_file_count)}')
